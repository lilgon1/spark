
pip install pyspark


import pyspark

-creating a spark context
from pyspark import SparkConf
from pyspark.context import SparkContext 
conf = SparkConf().setAppName('Pyspark').setMaster('local')
sc = SparkContext(conf=conf)

-function to calculate mod
def mod(x):
import numpy as np
return (x, np.mod(x,2)

-creating an rdd
rdd = sc.parallelize(range(1000)),map(mod.take(10)
print(rdd)

-creating rdd using list (use paralize for multiple values
values = [1,2,3,4,5]
rdd = sc.parallelize(values)

-printing the rdd valus
rdd.take(5)

-uploading a file to google collab
from google.colab import files
uploaded = files.upload()

-loading a textfile to spark
rdd= sc.textfile("Spark.text)

-print rdd data 
rdd.collect()

-rdd persistence
aba = sc.parallelize(range(1,10000,2))
aba.persist()

-rdd caching
textfile = sc.textFile("test.txt")
textfile.cache()

-map
x = sc.parallelize(["spark", "rdd", "example", "sample", "example"])
y = x.map(lambda x:(x,1))
y.collect()

-flatmap
rdd = sc.parallelize([2,3,4])
sorted(rdd.flatMap(lambda x: range(1,x)).collect())

-filter
rdd = sc.parallelize([1,2,3,4,5])
rdd.filter(lambda x: x%2 == 0).collect()

-sample
parrallel = sc.parallelize(range(9))
parrallel.sample(True, .2).count()

-sample
parrallel.sample(False,1).collect()

-union
parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.union(par).collect()

-intersection
parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.intersection(par).collect()

-distinct
parallel = sc.parallelize(range(1,9))
par = sc.parallelize(range(5,15))
parallel.distinctllect()

-sort by
y = sc.parallelize([5,7,1,3,2,1])
y.sortBy(lambda c: c, True).collect()

-ortby
z = sc.parallelize([("H", 10), ("A", 26),("F", 3) ,("Z", 1), ("L", 5)])
z.sortBy(lambda c: c, False).collect()

-map partitions
rdd = sc.parallelize([1,2,3,4], 2)
def f(iterator): yield sum(iterator)
rdd.mapPartitions(f).collect()

-map partition-wthin index
rdd = sc.parallelize([1,2,3,4], 4)
def f(splitIndex, Iterator): yield splitIndex
rdd.mapPartitionsWithIndex(f).sum()

-Group by
rdd = sc.parallelize([1,1,2,3,5,8])
result = rdd.groupBy(lambda x: x%2).collect()
sorted ([(x, sorted(y)) for (x,y) in result])

-key by
x = sc.parallelize(range(0,3)).keyBy(lambda x: x*x)
y = sc.parallelize(zip(range(0,5), range(0,5)))
[(x, list(map(list, y))) for x, y in sorted(x.cogroup(y).collect())]

-zip
x = sc.parallelize(range(0,5))
y = sc.parallelize(range(1000, 1005))
x.zip(y).collect()

#zip with index
sc.parallelize(["a", "b", "c", "d"], 3).zipWithIndex().collect()

-repartitioning
rdd = sc.parallelize([1,1,2,3,5,8])
sorted (rdd.glom().collect())
len(rdd.repartition(2).glom().collect())

-iterator
def f(iterator)
for x in interator
print(x)
sc.parallelize([1,2,3,4,5]).foreachPartition(f)
--------
math actions

sum()
mim()
max()
mean()
variance() - how much each value varies


-countby - groups all same and sorts them 
a = sc.parallelize([1,2,3,4,5,6,7,8,9,1,1,1,12,2,2,3,3,4,5,])
a.countByValue()

toDebugString()
used to debug

splitting words
words = noemptylines.flatmap(lambda x: x.split(' '))

counting frequency of each word
wc = words.map(lambda x: (x,1)).reduceByKey(lambda x,y: x+y).map(lambda x: (x[1], x[0])).sortByKey(false)
